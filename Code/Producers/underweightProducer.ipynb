{"cells":[{"cell_type":"code","source":["import requests\nimport json\nfrom pyspark.sql.types import *\n\nresponse = requests.get(\"https://ghoapi.azureedge.net/api/NCD_BMI_18A?$filter=SpatialDimType%20eq%20%27Country%27%20and%20Dim1%20eq%20%27BTSX%27\")\nadultUnderweight = response.json()['value']\ndfUnderweight = spark.createDataFrame(adultUnderweight, schema = StructType([StructField(\"SpatialDim\", StringType(), True),\n                                                                             StructField(\"TimeDim\", StringType(), True),\n                                                                             StructField(\"Dim1\", StringType(), True),\n                                                                             StructField(\"NumericValue\", StringType(), True)]))\n\ndfUnderweight = dfUnderweight.withColumn(\"TimeDim\", dfUnderweight[\"TimeDim\"].cast('int')) \\\n                             .withColumn(\"NumericValue\", dfUnderweight[\"NumericValue\"].cast('float')) \\\n                             .withColumnRenamed(\"TimeDim\",\"Year\") \\\n                             .withColumnRenamed(\"SpatialDim\",\"Country\") \\\n                             .withColumnRenamed(\"NumericValue\",\"PercentUnderweightAdults\") \\\n                             .drop(\"Dim1\")\n\ndisplay(dfUnderweight)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Import Statements and API Query","showTitle":true,"inputWidgets":{},"nuid":"eb316fb7-3ba9-4f2d-9470-27d91e165769"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# Make sure we have all the countries\ndfUnderweight.select(\"Country\").distinct().count()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"546903fa-c352-448f-8410-aaee1b36908d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: 195</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: 195</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# KAFKA PRODUCER\n\ndef error_cb(err):\n    \"\"\" The error callback is used for generic client errors. These\n        errors are generally to be considered informational as the client will\n        automatically try to recover from all errors, and no extra action\n        is typically required by the application.\n        For this example however, we terminate the application if the client\n        is unable to connect to any broker (_ALL_BROKERS_DOWN) and on\n        authentication errors (_AUTHENTICATION). \"\"\"\n\n    print(\"Client error: {}\".format(err))\n    if err.code() == KafkaError._ALL_BROKERS_DOWN or \\\n       err.code() == KafkaError._AUTHENTICATION:\n        # Any exception raised from this callback will be re-raised from the\n        # triggering flush() or poll() call.\n        raise KafkaException(err)\n\n\ndef acked(err, msg):\n    \"\"\" \n        Error callback is used for generic issues for producer errors. \n        \n        Parameters:\n            err (err): Error flag.\n            msg (str): Error message that was part of the callback.\n    \"\"\"\n    if err is not None:\n        print(\"Failed to deliver message: %s: %s\" % (str(msg), str(err)))\n    else:\n        print(\"Message produced: %s\" % (str(msg)))\n        \n#DO NOT DELETE THIS\nfrom time import sleep\nimport uuid\nfrom confluent_kafka import Producer, Consumer, KafkaError, KafkaException\nimport json\nfrom confluent_kafka.admin import AdminClient, NewTopic\n\n\n#KAFKA variables, Move to the OS variables or configuration\n# This will work in local Jupiter Notebook, but in a databrick, hiding config.py is tougher. \nconfluentClusterName = \"stage3talent\"\nconfluentBootstrapServers = \"pkc-ldvmy.centralus.azure.confluent.cloud:9092\"\nconfluentTopicName = \"who-underweight-final\"\nschemaRegistryUrl = \"https://psrc-gq7pv.westus2.azure.confluent.cloud\"\nconfluentApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentSecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\nconfluentRegistryApiKey = \"YHMHG7E54LJA55XZ\"\nconfluentRegistrySecret = \"/XYn+w3gHGMqpe9l0TWvA9FznMYNln2STI+dytyPqtZ9QktH0TbGXUqepEsJ/nR0\"\n\nadmin_client = AdminClient({\n    'bootstrap.servers': confluentBootstrapServers,\n    'sasl.mechanism': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': confluentApiKey,\n    'sasl.password': confluentSecret,\n    'group.id': str(uuid.uuid1()),  # this will create a new consumer group on each invocation.\n    'auto.offset.reset': 'earliest',\n    'error_cb': error_cb,\n})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Initialize producer","showTitle":true,"inputWidgets":{},"nuid":"8a8109f5-8c3c-44d1-9a2d-1de6abed2c0b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["# #ONLY NEEDS TO BE RUN ONCE\n# topic_list = []\n\n# topic_list.append(NewTopic(confluentTopicName, 1, 3))\n# admin_client.create_topics(topic_list)\n# futures = admin_client.create_topics(topic_list)\n\n\n# try:\n#     record_metadata = []\n#     for k, future in futures.items():\n#         # f = i.get(timeout=10)\n#         print(f\"type(k): {type(k)}\")\n#         print(f\"type(v): {type(future)}\")\n#         print(future.result())\n\n# except KafkaError:\n#     # Decide what to do if produce request failed...\n#     print(traceback.format_exc())\n#     result = 'Fail'\n# finally:\n#     print(\"finally\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create topic (commented out)","showTitle":true,"inputWidgets":{},"nuid":"bf602721-617c-4d64-8665-ce3f0e3d3528"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["p = Producer({\n    'bootstrap.servers': confluentBootstrapServers,\n    'sasl.mechanism': 'PLAIN',\n    'security.protocol': 'SASL_SSL',\n    'sasl.username': confluentApiKey,\n    'sasl.password': confluentSecret,\n    'group.id': str(uuid.uuid1()),  # this will create a new consumer group on each invocation.\n    'auto.offset.reset': 'earliest',\n    'error_cb': error_cb,\n})\n\nDF_Pandas = dfUnderweight.toPandas()\n\nfor i in range(len(DF_Pandas)):\n    p.produce(confluentTopicName,DF_Pandas.iloc[i].to_json())\n    p.flush()\n    sleep(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Create and Run Producer","showTitle":true,"inputWidgets":{},"nuid":"374cdef9-d67f-4762-bfe9-ba1354655456"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"underweightProducer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":164542636741876}},"nbformat":4,"nbformat_minor":0}
